# Online reports and narrations of flood events

```{r setup, message=FALSE,warning=FALSE}
library(glitter)
library(tidyverse)
library(ggpubr)
library(tidytext)
library(leaflet)
wd_events=readRDS("data/wd_events.RDS")
wm_full=readRDS("data/wm_full.RDS")
wp_20_words_per_event=readRDS("data/wp_20_words_per_event.RDS")
wm_full=wm_full %>% 
  left_join(wp_20_words_per_event, by=c("flood","flood_label"))
dfo_comp=readRDS("data/dfo_comp.RDS")
wm_comp=readRDS("data/wm_comp.RDS")
wm_dfo=readRDS("data/wm_dfo.RDS")
wm_dfo_corr=readRDS("data/wm_dfo_corr.RDS")
```

## Aim of the article

This study's objectives are to

1)  collect and assess the information provided by the Wikimedia initiative (Wikidata, Wikipedia, Wikimedia Commons) on flood events.
2)  compare this data, which is crowdsourced, to an institutionnally curated database on the same topic.
3)  use this data to characterize and understand (in terms of human impact especially) the flood events documented.

# Introduction

## Why is it complicated to produce worldwide data about floods

Producing a worldwide dataset about flood events poses numerous challenges due to the complexity of semantics, collection, standardization, and accessibility of such data. One primary obstacle is the lack of uniform reporting standards leading to inconsistencies in definitions of flood events and in data collection methods. Additionally, varying technological capabilities and resources across geographical regions of the world contribute to disparities in data availability. Natural disasters like floods might occur in areas where communication and infrastructure are limited, hindering real-time data collection. Political, social, and economic factors also play a role, as some regions may under-report or inadequately document flood occurrences due to reasons such as insufficient resources, lack of governmental transparency, or competing priorities. Despite advancements in satellite technology and remote sensing, ensuring comprehensive, accurate, and up-to-date global flood data remains a persistent challenge that requires huge collaborative efforts, data curating, and improved global information-sharing mechanisms.

## Participatory local data about floods

Participatory data collection holds significant potential in enhancing the understanding and management of flood events. This approach involves engaging local communities, citizens, and grassroots organizations in the collection, analysis, and sharing of data related to flood occurrences. Examples of such initiatives show the potential of participatory data to complement institutional data about floods at a local scale [@dixon_role_2021, @sekajugo_can_2022]. By incorporating local knowledge, experiences, and observations, participatory methods complement traditional data sources, offering valuable insights into flood-prone areas that might be overlooked by centralized or remote monitoring systems. More broadly, volunteered geographical data at a local scale has been shown to be a valuable source of data regarding natural hazards.

Communities living in flood-prone regions possess unique, context-specific knowledge about local environmental changes, historical flood patterns, vulnerable areas, and coping mechanisms. Engaging these communities in data collection through citizen science initiatives, mobile applications, community mapping, or participatory workshops allows for the collection of granular, real-time information that supplements existing datasets. This bottom-up approach not only enhances the accuracy and granularity of flood data but also fosters community empowerment, resilience-building, and local capacity development.

Moreover, participatory data can bridge gaps in official reporting by capturing small-scale or localized flood events that might not meet the threshold for formal reporting. Integrating participatory data with conventional datasets enables a more comprehensive understanding of flood dynamics, aiding in the development of effective early warning systems, disaster response plans, and mitigation strategies.

However, challenges such as ensuring data accuracy, reliability, and standardization persist in participatory approaches. Establishing protocols for data validation, quality control, and harmonization between participatory and formal datasets is crucial to maximize the potential of participatory data while maintaining data integrity [@senaratne_review_2017]. The stakes around data validation and homogeneization are particularly high in the prospect of studying floods at a large (or even global) scale.

Overall, leveraging participatory data in conjunction with conventional methods has the potential to enrich global flood databases, improve resilience, and empower communities to better respond to and mitigate the impacts of flood events. This collaborative approach can contribute significantly to more holistic and inclusive strategies for managing floods worldwide.

## Institutional global data about floods

The Dartmouth Flood Observatory (DFO) is a research group specializing in the collection and analysis of global flood data. Combining remote sensing and news reports' analysis, the DFO produces detailed information on the extent, frequency, and impacts of floods worldwide [@kundzewicz_large_2013]. It constitutes an invaluable source of information on the subject, used in many scientific studies on flood events.

## Wikimedia data

In parallel to that kind of research-related, expert database on the subject, the Wikimedia project stands as a remarkable source of structured or semi-structured participatory data with its vast array of user-generated content across platforms like Wikipedia, Wikidata, and Wikimedia Commons. Wikipedia, being one of the largest collaborative encyclopedias globally, harnesses the collective knowledge of volunteers worldwide who contribute, edit, and curate articles on diverse subjects, including geographical features, history, and environmental events such as floods. It can be viewed both as a source of breaking news and as an encyclopedia, providing information and narrative regarding ongoing and past events, in particular crises and controversies [@moats_following_2019]. Wikidata, an open database, provides structured data that can be used to categorize and link information, potentially cataloging flood events, affected regions, and relevant details. Wikimedia Commons serves as a repository for multimedia files, housing images, maps, and other visual resources related to floods, contributing to a more comprehensive understanding of such events. The open nature of Wikimedia projects allows for continual updates and contributions, making it a valuable resource for researchers, policymakers, and the public seeking information on various topics, including flood events, across the globe.

## Wikimedia for crisis management

# Methods

The methods are detailed (for now) in the other pages of this document.

## Primary census of flood events

We collect Wikidata about floods, using the glitter R package (an API-client to the Wikidata SPARQL endpoint API) [@vaudor_glitter_2023].

## Description of events

We complete the database regarding flood events with attributes corresponding to

-   location
-   country
-   coordinates
-   deathtoll
-   related Wikipedia articles
-   linked images

## Comparison to an institutional database

We compare this data to the DFO database, trying to establish a link between events described on both datasets, based on distance and time gap.

# Results

We have gathered tables of data organized as follows:

-   wd_events: a table with one row= one event documented on Wikidata
-   wp_pages: a table with one row= one page in Wikipedia (each one relative to a flood in wd_events), with attributes regarding each page and complemented by
    -   a column `text` corresponding to the text, and
    -   a column `textt` corresponding to the text translated in English
-   wp_revisions: a table with one row= one revision of a Wikipedia page

```{r wde}
wd_events=readRDS("data/wd_events.RDS")
Nwde=wd_events %>% nrow()
```

The initial query provided **`r Nwde` Wikidata flood events**.

```{r wpp}
wp_pages=readRDS("data/wikisites_translated.RDS")
Nwpp=nrow(wp_pages)
```

```{r wpp_describe}
Nwde_with_wpp=wp_pages %>% 
  group_by(flood) %>% 
  tally() %>% 
  nrow()
```

There are **`r Nwpp` Wikipedia pages** associated to these events (i.e. on average `r round(Nwpp/Nwde,2)` Wikipedia pages per event). `r Nwde_with_wpp` flood events (`r round(Nwde_with_wpp/Nwde*100,2)`%) are associated to at least one Wikipedia page.

```{r wpp_describe_text}
wp_pages=readRDS("data/wp_pages.RDS") %>% 
  mutate(length=map_dbl(textt,str_length))
Nwpp_en=wp_pages %>% filter(lang=="en") %>% nrow()
Nwpp_untranslated=wp_pages %>% 
  filter(is.na(textt)) %>% 
  nrow()
```

`r Nwpp_en` Wikipedia pages (`r round(Nwpp_en/Nwpp*100,2)`%) are in English. Out of the `r Nwpp-Nwpp_en` remaining, we could translate most pages to English, except `r Nwpp_untranslated` pages which were in languages that Google Translate did not support.

```{r wpp_length}
wpp_length_average=wp_pages %>% pull(length) %>% mean(na.rm=TRUE)
wpp_length_quant10=wp_pages %>% pull(length) %>% quantile(0.1,na.rm=TRUE)
wpp_length_quant90=wp_pages %>% pull(length) %>% quantile(0.9,na.rm=TRUE)
```

The average length of articles (in English or translated to English) is `r round(wpp_length_average)` characters. The 10% shortest articles represent less than `r round(wpp_length_quant10)` characters and the 10% longest ones are more than `r round(wpp_length_quant90)` characters long.

```{r wpw}
wp_words=readRDS("data/wp_words.RDS")
#Nwpw=
```

```{r wpr}
wp_revisions=readRDS("data/wp_revisions.RDS")
Nwpr=nrow(wp_revisions)

wp_revisions=wp_revisions %>% 
  mutate(bot=stringr::str_detect(user_name,"B|bot"))
Nwpr_humans=wp_revisions %>% 
  filter(!bot) %>% 
  nrow()
```

There are **`r Nwpr` revisions** that produced the Wikipedia pages as they appear to this date, which represents an average `r round(Nwpr/Nwpp)` revisions per page.

`r Nwpr_humans` (`r round(Nwpr_humans/Nwpr*100,2)`%) of these revisions appear to have been done by human editors and not bots (Wikipedia usernames do not contain the strings "bot" or "Bot").

```{r Nwpu_Nwpuh}
Nwpu=wp_revisions %>%
  select(user_name) %>%
  unique() %>% 
  nrow()
Nwpuh=wp_revisions %>% 
  filter(!bot) %>% 
  select(user_name) %>%
  unique() %>% 
  nrow()
```

There are **`r Nwpu` distinct editors**, `r Nwpuh` (`r round(Nwpuh/Nwpu*100,2)`%) of which appear to be humans. On average, each human editor in our corpus is responsible for `r round(Nwpr_humans/Nwpuh,2)` edits. The number of articles (in the corpus) edited by each human editor is distributed as follows:

```{r wp_revisions}
wp_revisions %>% 
  filter(!bot) %>% 
  group_by(user_name) %>% 
  summarise(number_of_articles_edited=n_distinct(article),.groups="drop") %>%
  mutate(nusertot=n()) %>% 
  mutate(number_of_articles_edited=cut(number_of_articles_edited,
                                       c(0,1,2,5,10,100,400))) %>% 
  group_by(number_of_articles_edited,nusertot) %>% 
  summarise(number_of_editors=n(),.groups="drop") %>% 
  mutate(proportion_of_editors=round(number_of_editors/nusertot*100,2)) %>% 
  select(number_of_articles_edited,number_of_editors,proportion_of_editors) %>% 
  knitr::kable()
```

## Wikidata on floods

```{r wd_stats}
wm_full=readRDS("data/wm_full.RDS")
```

```{r wd_years, fig.width=8,fig.height=4}
#| label: fig-wd_year
#| fig-cap: "Distribution of flood events through time A) all events dated with a year-accuracy B) events dated with a year-accuracy starting in 2000"
wd_events_freq=wd_events %>% 
  mutate(cat_year=cut(year,breaks=c(1000,1200,1400,1600,1800,1950,
                                    2000,2023),dig.lab=10)) %>% 
  group_by(cat_year) %>% tally()
p1=ggplot(wd_events_freq, aes(x=cat_year,y=n))+
  geom_col()
p2=ggplot(wd_events %>% filter(year>=2000), aes(x=year))+
  geom_histogram(breaks=2000:2023)
figure <- ggarrange(p1,p2,
                    labels = c("A", "B"),
                    ncol = 1, nrow = 2)
figure
```

```{r Nwd_in_range}
Nwd_in_range=wd_events %>% 
  filter(year>=1985 & year<=2021) %>% 
  sf::st_drop_geometry() %>%
  summarise(n=length(unique(flood))) %>% 
  pull(n)
```

Based on the dates of observations for the DFO data base, `r Nwd_in_range` out of the `r nrow(wd_events)` flood events in our Wikidata base might fall into it.

## Compare to DFO data

```{r nb_corr}
nb_wm=wm_comp %>% group_by(flood) %>% tally() %>% nrow()
nb_wm_in_dfo=wm_dfo_corr %>% filter(!is.na(floodcorr)) %>% group_by(flood) %>% tally() %>% nrow()
nb_dfo=dfo_comp %>% group_by(flood) %>% tally() %>% nrow()
nb_dfo_in_wd=wm_dfo_corr %>% group_by(floodcorr) %>% tally() %>% nrow()
```

Out of `r nb_wm` flood events documented in the WD comparison data subset, `r nb_wm_in_dfo` have at least one correspondence in the DFO comparison data subset (`r round(nb_wm_in_dfo/nb_wm *100,2)`%).

Out of `r nb_dfo` flood events documented in the DFO comparison data subset, `r nb_dfo_in_wd` have at least one correspondence in the WD comparison data subset (`r round(nb_dfo_in_wd/nb_dfo *100,2)`%).

## Wikipedia content

### News or encyclopedia?

```{r delay_revisions,fig.width=10,fig.height=8, warning=FALSE, message=FALSE}
#| label: fig-delay_revisions
#| fig-cap: "Time delay (in years) between events and revisions to the Wikipedia articles documenting them. The color indicates the precision of the event date as stated by Wikidata (it can be accurate to the day, month or year)."
flood_date_precision=readRDS("data/wm_full.RDS") %>% select(flood,date_precision) %>% unique()
truc=wp_revisions %>%
  left_join(wd_events %>% select(flood,date),by="flood") %>%
  left_join(flood_date_precision,by="flood") %>% 
  mutate(timestamp=as.POSIXct(timestamp)) %>% 
  mutate(timestamp=lubridate::ymd(timestamp)) %>% 
  mutate(diff=difftime(timestamp,date,units="days")) %>% 
  mutate(delay=as.numeric(diff)) %>% 
  mutate(delay=delay/365.25) %>% 
  mutate(year=lubridate::year(date)) %>% 
  mutate(year=cut(year,c(-Inf,2000,2004,2008,2012,2018,2022,2024))) %>% 
  mutate(delay=cut(delay,c(-Inf,0,0.125,0.25,0.5,1,2,5,10,15,25,+Inf))) %>% 
  group_by(year) %>% 
  mutate(ntot=n()) %>% 
  ungroup() %>% 
  group_by(year,delay,ntot,date_precision) %>% 
  summarise(n=n(),.groups="drop") %>% 
  mutate(prop=n/ntot) %>% 
  na.omit()
ggplot(truc ,aes(x=delay,y=prop,fill=date_precision))+
  geom_col()+
  facet_grid(rows=vars(year))
```

@fig-delay_revisions shows that most of the writing of Wikipedia articles related to flood events take place during the month of their occurrence (except of course when Wikipedia did not exist (before 2001) or when it was still moderately used to document current events. This highlights the fact that Wikipedia articles about recent flood events can be viewed as news rather than encyclopedic reports on past events. Conversely, one can argue that articles exhibiting a large proportion of revisions implemented a long time after the occurrence of the event could be indicative of an ongoing debate or controversy, or be the ongoing focus of some social or human interest.

### Topics

We applied a structural topic modelling (STM) algorithm to identify topics in the Wikipedia text corpus and selected a number of topics of 6 [@weston_selecting_2023]. The STM method provides topics characterized by a set of most probable words (see fig. @fig-topic_terms). To help with the interpretation of subsequent results and figures, and although the corpus does not exhibit very distinct topics (which is unsurprising given the inherent specificity and formal homogeneity of the corpus) we propose a short label for each topic ("spatial range", "weather", "hydrology", "management", "historical importance", "human toll")

```{r get_text_data}
topic_model=readRDS("data/topic_model.RDS")
termes_thematiques=readRDS("data/termes_thematiques.RDS")
tib_gamma=readRDS("data/tib_gamma.RDS")
```

```{r plot_topic_terms,fig.width=10,fig.height=10, warning=FALSE, message=FALSE}
#| label: fig-topic_terms
#| fig-cap: "Topics identified by an STM process for the Wikipedia articles' corpus"
ggplot(termes_thematiques  %>%
         mutate(topic=as.factor(topic_label)) %>%
         mutate(term=reorder_within(term,by=beta,within=topic)),
       aes(x=beta,y=term, fill=topic_label))+
  geom_bar(stat="identity")+
    facet_wrap(facets=vars(topic), scales="free")+
    theme(legend.position="none")+
  scale_y_reordered()
```

```{r tib_meta_thematiques}
tib_meta_thematiques=wp_words %>% 
  group_by(flood,flood_label) %>% 
  summarise(n=sum(n),.groups="drop") %>% 
  mutate(length=case_when(n>median(n)~"long document",
                          n<=median(n)~"short document")) %>% 
  left_join(tib_gamma, by="flood",relationship = "many-to-many") %>%
         group_by(topic,topic_label,length) %>% 
         summarise(sgamma=sum(gamma),.groups="drop") %>% 
         ungroup() %>% 
         na.omit()
```

```{r plot_thematiques_according_to_length,fig.width=5,fig.height=4}
#| label: fig-topic_doctype
#| fig-cap: "Topics identified by an STM process for the Wikipedia articles' corpus"
ggplot(tib_meta_thematiques,
       aes(x=forcats::fct_reorder(topic_label,desc(topic_label)),y=sgamma,fill=topic_label))+
  geom_col()+
  facet_grid(cols=vars(length))+
  coord_flip()+
  theme(legend.position="none")+
  labs(x="topic_label")
```

@fig-topic_doctype shows that the length of the article is probably a very important factor in the topics addressed (or conversely, the topics addressed determine the length of the article), with shorter articles corresponding to factual statements regarding spatial range and hydrology, and bringing justification of the existence of the article through a discussion of the historical importance of the event, while longer articles might convey more extensive reports of weather conditions and management of the crisis.

## Map

```{r leaf_wm_map, fig.width=10,fig.height=10, warning=FALSE, message=FALSE}
wm_map=readRDS("data/wm_map.RDS")
# Définition d'une échelle colorée 
# (en fonction de date de sortie) 
pal <- colorNumeric(c("red", "green", "blue"),
                    c(1648,1900,1950,1980,2000,2010,2023)) 
# Création de la carte 
leaf_wm_map=leaflet(wm_map) %>% # déf carte 
  addProviderTiles(providers$Esri.WorldTopoMap) %>% # ajout fond de carte
  addCircleMarkers(col=~pal(year),
                   popup = ~popup,
                   radius =~log(nwords+1)
                   ) 
leaf_wm_map
```

## Map comparison to DFO

We tried to find the correspondence between events documented in Wikidata and events documented in the DFO dataset. To do that, for each WD event, we searched for all events in the DFO dataset occurring less than a year and 30 days apart and less than 400 kms away OR occurring less than a year and 30 days apart and in the same country. The limit of one year and 30 days accounts for the sometimes low precision of date (e.g., year) in Wikidata.

```{r map_comp, warning=FALSE, message=FALSE, fig.width=10, fig.height=12}
library(leaflet) 
# Définition d'une échelle colorée 
# (en fonction de date de sortie) 
# Création de la carte 
joined_events=readRDS("data/joined_events.RDS")
pal=colorFactor(c("red","blue"), domain=c("wd","dfo"))
comp_map=leaflet(wm_dfo)  %>%
  addProviderTiles(providers$Esri.WorldTopoMap) %>% # ajout fond de carte
  addCircleMarkers(col=~pal(source),
                   popup =~popup,
                   radius=3) %>% 
  addPolylines(data=joined_events,color="green")
  

```

```{r}
#| label: fig-map_comp
#| fig-cap: "Comparison of events defined in DFO (red) and Wikidata/Wikipedia (blue). The attempts at finding the correspondence between a Wikidata/Wikipedia event and a corresponding one in the DFO dataset correspond to the lines."
comp_map
```

-   oui je sais ces résultats sont assez vilains pour le moment. Je vais essayer de trouver une meilleure représentation + il faudra discuter les raisons pour lesquelles il est difficile de faire une correspondance -définition des événements différent, par exemple souvent dans Wikipedia plusieurs événements sont regroupés par exemple "Inondations de telle année en Europe centrale"-) \*

# Discussion

This study demonstrates the possibility to list and collect information on flood events on a large temporal and spatial scale through Wikidata, Wikimedia and Wikipedia projects. Although these data exhibit some heterogeneity [@lorini_uneven_2020; @ruprechter_poor_2023] due to economic and digital inequities, being able to rely on a preexistent, rich global dataset spanning a large time period is still a priceless asset in studying floods, and could be a first step to better-quality curated datasets. Besides, large-scale institutional data sources themselves are not immune to spatial heterogeneities due to scarcity of research activity in some areas, linguistic barriers, varying degrees of public investment in disaster management, or other sources of environmental, political and social inequities.

Linked Open Data such as Wikidata hold great interest for the study of natural or social events due to their inherent ability to offer contextual information to documented items. The interconnected nature of Wikidata's model (as a knowledge graph) as well as their link to other data silos indeed allow users to easily access additional information or related resources linked to a particular event. The present study hence demonstrates the strength of such a data source to provide context (in particular through the integration of Wikipedia discourse elements and Wikimedia Commons images) as well as the difficulties inherent to identifying particular flood events based solely on presumably objective information, such as coordinates and dates for these events.
