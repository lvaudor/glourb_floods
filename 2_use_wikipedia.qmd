# Use Wikipedia to complete Wikidata

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(rainette)
wd_raw=readRDS("data/wd_raw.RDS")
```

We translate all Wikipedia pages' titles and texts to English (when Google translate allows it).

```{r }
if(!file.exists("data/wikisites_translated.RDS")){
  wikisites=readRDS("data/wikisites.RDS") %>% 
     mutate(lang=stringr::str_extract(article,"(?<=https://)([[:alpha:]]|-)*(?=\\.)"))%>%
     mutate(lang=case_when(lang=="simple"~"en",
                           TRUE~lang)) %>% 
     mutate(title=stringr::str_extract(article,"(?<=\\/wiki\\/).*")) %>% 
     mutate(title=purrr::map_chr(title,URLdecode)) %>% 
     mutate(title=stringr::str_replace_all(title,"_"," ")) %>% 
     mutate(translated_title=purrr::map2(title,lang,
                                         ~safely(polyglotr::google_translate)(.x,,source_language=.y,target_language="en"))) %>% 
    mutate(translated_title=purrr::map(translated_title,
                                       ~.$result)) %>%
    mutate(translated_title=purrr::map_chr(translated_title,
                                           ~replace(.x,is.null(.x),NA)))
  saveRDS(wikisites,"data/wikisites_translated.RDS")
}
wikisites=readRDS("data/wikisites_translated.RDS")%>% 
  filter(flood %in% wd_raw$flood)
```

We define a translate function, based on the polyglotr package.

```{r translate_function}
translate <- function(i){
    text=wikisites_text$text[i][[1]]
    if(wikisites_text$lang[i]=="en"){return(text)}
    tryCatch(
        #try to do this
        {texts=unlist(stringr::str_split(text,"\n"))
         result=purrr::map_chr(texts,
                               polyglotr::google_translate,
                               source_language=wikisites_text$lang[i])
         result=paste0(result, collapse="\n")
         return(result)
        },
        #if an error occurs, tell me the error
        error=function(e) {
            message('An Error Occurred')
            print(e)
            return(NA)
        },
        #if a warning occurs, tell me the warning
        warning=function(w) {
            message('A Warning Occurred')
            print(w)
            return(NA)
        }
    )
}
```

## is language local?

```{r def_is_language_local}
is_language_local=function(lang_id,data){
  result=countries %>% 
    filter(country %in% data$country) %>% 
    filter(lang_id==language_code)
  result=case_when(nrow(result)>=1~TRUE,
                   nrow(result)==0~FALSE)
  return(result)
}

countries=readRDS("data/countries.RDS") %>% 
  select(country,language_code) %>% 
  unique()
wp_pages=readRDS("data/wp_pages.RDS") %>% 
  unique()
if(!("local" %in% colnames(wp_pages))){
    wd_events=readRDS("data/wd_events.RDS") 
    wp_pages_local_articles=wd_events %>% select(flood,country) %>% 
      group_by(flood) %>% 
      tidyr::nest() %>% 
      left_join(wp_pages %>% select(flood,article,lang),
                by="flood",relationship="many-to-many") %>% 
      mutate(local=purrr::map2_lgl(lang,data,is_language_local)) %>% 
      select(article,local)
    wp_pages=wp_pages %>% 
      left_join(wp_pages_local_articles,by=c("flood","article"))
    saveRDS(wp_pages,"data/wp_pages.RDS")
}
```

## clean text

```{r def_clean_banners}
clean_banners=function(text){
  banner=str_detect(text,"This article|this banner|your knowledge")
  if(banner & !is.na(text)){
  result=text %>% 
    str_replace("This article is.*\\n","") %>% 
    str_replace("This article is a draft concerning.*\\.\\n\\n","") %>% 
    str_replace("\\nedit - edit code - edit Wikidata\\n","") %>% 
    str_replace("You can share your knowledge by.*\\.","") %>% 
    str_replace("This article is based.*$","") %>% 
    str_replace("^(.|\\n)*If you think these points have been resolved, you can remove this banner and improve the formatting of another article\\.","")%>% 
    str_replace("This .*article.*\\. You can help Wikipedia by expanding it\\.","") %>% 
    str_replace("\\..*\\}","")
  }else{result=text}
  return(result)
}
```

```{r apply_translation}
if(!file.exists("data/wikisites_text.RDS")){
  library(rvest)
  # Get text paragraphs from all Wikipedia articles

  get_text=function(html){
    html_nodes(html,"h1, h2, h3, h4, h5, h6, p") %>%
    purrr::map(html_text) %>% 
    stringr::str_replace("\\[.*\\]$","")
  }
  wikisites_text=wikisites %>%
    mutate(html=purrr::map(article,read_html)) %>% 
    mutate(text=purrr::map(html,safely(get_text))) %>% 
    mutate(text=purrr::map(text,"result")) %>% 
    select(-html) %>% 
    mutate(text=purrr::map(text,~paste0(.x,collapse="\n"))) 

  
  # Translate them all to English with Google Translate (if possible)
  wikisites_textt=wikisites_text %>% 
    mutate(textt=NA)
  tmp=Sys.time()
  for(i in 1:nrow(wikisites_textt)){
    print(i)
    result=translate(i)
    if(length(result)>0){
    wikisites_textt$textt[i]=result}
  }  
  print(Sys.time()-tmp) 
saveRDS(wikisites_textt,"data/wp_pages_new.RDS")

  wp_pages=wikisites_textt %>% 
    mutate(textt=purrr::map_chr(textt,clean_banners)) %>% 
    mutate(textt=stringr::str_replace(textt,"mw-parser-output.*$","")) %>% 
    mutate(length=map_dbl(textt,str_length)) %>% 
    select(-flood_label)
  saveRDS(wp_pages,"data/wp_pages_new.RDS")
  
##FAIT JUSTE AVANT VACANCES => wp_pages_new.RDS
}



wp_pages=readRDS("data/wp_pages.RDS")
```

## wp_words

```{r wikisites_text_to_words}
if(!file.exists("data/wp_words.RDS")){
  library(mixr)
  lexicon_en=get_lexicon("en")
  wp_words=wp_pages %>%
    tidytext::unnest_tokens(output="word",input="textt",token="words") %>%
    left_join(lexicon_en,by=c("word")) %>%
    filter(type!="sw" & lemma!="flood") %>%
    group_by(flood,lemma) %>%
    summarise(n=n(),.groups="drop") %>%
    arrange(flood,desc(n)) 

wp_words_spec <- mixr::tidy_specificities(wp_words %>% filter(n>3),lemma, flood) %>% 
  filter(spec>2) %>% 
  group_by(flood) %>% 
  summarise(specific_words=paste(lemma,collapse="; "),.groups="drop")
saveRDS(wp_words_spec,"data/wp_words_spec.RDS")
saveRDS(wp_words,"data/wp_words.RDS")
wp_20_words_per_event=wp_words %>% 
  group_by(flood) %>% 
  arrange(desc(n)) %>% 
  slice_head(n=20) %>% 
  summarise(words=paste(lemma,collapse="; "), .groups="drop")
saveRDS(wp_20_words_per_event,"data/wp_20_words_per_event.RDS")
}
wp_words=readRDS("data/wp_words.RDS") %>% 
  filter(flood %in% wd_raw$flood)
```

## Classification of segment's topics

```{r prepare_class_rainette}
if(!file.exists("data/wp_segments.RDS")){
  library(quanteda)
  library(rainette)
  library(mixr)
    lexicon_en=get_lexicon("en")
    wp_segments=readRDS("data/wp_pages.RDS") %>%
    tidytext::unnest_tokens(output="word",input="textt",token="words") %>%
    left_join(lexicon_en,by=c("word")) %>%
    mutate(keep=(type!="sw" & lemma!="flood")) %>% 
    group_by(flood,flood_label,article) %>%
    mutate(lemma=case_when(is.na(lemma)|(!keep)~"",
                           TRUE~lemma)) %>% 
    mutate(keep=as.numeric(keep)) %>% 
    mutate(num_lemma=case_when(is.na(keep)~0,
                     TRUE~keep)) %>% 
    mutate(num_lemma=cumsum(num_lemma)) %>% 
    mutate(num_segment=ceiling(num_lemma/10+0.000001)) %>% 
    ungroup() %>% 
    group_by(flood,flood_label,article,num_segment) %>% 
    summarise(text_all=paste0(word, collapse=" "),
              text_sig=paste0(lemma,collapse=" "),
              .groups="drop") %>% 
    mutate(num_segment=paste0(article,"_",num_segment)) %>% 
    mutate(text_sig_dontkeep=text_sig)

  corpus=corpus(wp_segments,docid_field="num_segment",text_field="text_sig_dontkeep")
  tok <- tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE)
  dtm <- dfm(tok)
  dtm <- dfm_trim(dtm, min_docfreq = 30)
  set.seed(123)
  res=rainette(dtm,k=10,min_split_members=10)
  saveRDS(res,"data/res_rainette_wp.RDS")
  saveRDS(corpus,"data/corpus.RDS")
  saveRDS(dtm,"data/dtm.RDS")
}
res=readRDS("data/res_rainette_wp.RDS")
corpus=readRDS("data/corpus.RDS")
dtm=readRDS("data/dtm.RDS")
#rainette_explor(res,dtm,corpus)
```

```{r plot_class_rainette, fig.width=10, fig.height=8}
rainette_plot(
  res, dtm, k = 6,
  n_terms = 20,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 12
)
```

```{r attribute_classes}
if(!file.exists("data/wp_segments.RDS")){
  corpus$class=paste0("class_",cutree_rainette(res, k = 6))
  
  tib_classes=tibble::tribble(~class,~class_name,~color,
                  "class_1","governance","#4E79A7",
                  "class_2","relief","#F28E2B",
                  "class_3","damage","#E15759",
                  "class_4","hydrology", "#76B7B2", 
                  "class_5","anticipation","#59A14F",
                  "class_6","weather","#EDC948")
  wp_segments=docvars(corpus) %>% 
    left_join(tib_classes,by="class") %>% 
    mutate(class=as.factor(class),
           class_name=as.factor(class_name)) %>% 
    select(-flood,-flood_label)
  saveRDS(wp_segments,"data/wp_segments.RDS")
}
```

## add classif results summary to wp_pages

```{r make_wp_pages_class}
wp_segments=readRDS("data/wp_segments.RDS")
wp_segments_summary=wp_segments %>% 
  group_by(article,class,class_name,color) %>% 
  tally()
spec=mixr::tidy_specificities(wp_segments,
                              article,
                              class_name)
wp_pages_class=wp_pages %>% 
  left_join(wp_segments_summary,
            by="article") %>% 
  left_join(spec,by=c("article","class_name","n"))

saveRDS(wp_pages_class,"data/wp_pages_class.RDS")
```

## wp_revisions

```{r def_get_revs}
source("scripts/get_revs.R")
```

```{r apply_get_revs}
if(!file.exists("data/wp_revisions.RDS")){
library(httr)
wp_revisions=wp_pages %>% 
  select(-text,-textt) %>% 
  mutate(revisions=purrr::map2(lang,title,get_revs)) %>% 
  tidyr::unnest(cols=c(revisions)) %>% 
  select(-flood,-flood_label,-lang,-title,-translated_title)
saveRDS(wp_revisions,"data/wp_revisions.RDS")
}
```

# Get consultation stats on all pages

```{r get_page_views}
get_page_views=function(title,lang){
  # DÃ©finir la plage de dates pour laquelle vous souhaitez rÃ©cupÃ©rer les statistiques
  start_date <- "20000101"
  end_date <- "20240131"
  title=URLencode(title)
  string1="https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article"
  string2="all-access/all-agents"
  # Construire l'URL de l'API Wikimedia pour rÃ©cupÃ©rer les statistiques de consultation
  url=glue::glue("{string1}/{lang}.wikipedia/{string2}/{title}/daily/{start_date}/{end_date}")
  response= httr::GET(url)
  
  # VÃ©rifier si la requÃªte a rÃ©ussi
  if (httr::status_code(response) == 200) {
    # Convertir les donnÃ©es JSON en un objet R
    content=httr::content(response, "text", encoding = "UTF-8")
    data_views = jsonlite::fromJSON(content)[[1]] %>% 
      dplyr::select(timestamp,views)
  } else {
    cat("Erreur lors de la rÃ©cupÃ©ration des donnÃ©es.")
    cat(title)
    cat(lang)
    return(NA)
  }  
  return(data_views)
}
```

```{r wp_views_monthly}
if(!file.exists("data/wp_views_monthly.RDS")){
  wp_pages_curation=readRDS("data/wp_pages_curation.RDS")
  wp_views_daily = wp_pages_curation %>%
    dplyr::select(article,title,lang) %>% 
    dplyr::mutate(data=purrr::map2(.x=title,.y=lang, get_page_views)) %>% 
    tidyr::unnest(c(data)) %>% 
    mutate(timestamp=stringr::str_replace(timestamp,"00$","")) %>% 
    mutate(day=lubridate::ymd(timestamp)) %>% 
    mutate(month=round_date(day,"month"))
  saveRDS(wp_views_daily,"data/wp_views_daily.RDS")
  wp_views_monthly=wp_views_daily %>% 
    group_by(article,title,lang,month) %>%
    summarise(nviews=sum(views),.groups="drop") 
  saveRDS(wp_views_monthly, "data/wp_views_monthly.RDS")
}
```

```{r wp_pages_complete}
if(!file.exists("data/wp_pages_complete.RDS")){
    wp_views_monthly=readRDS("data/wp_views_monthly.RDS")
    wp_views_tot=wp_views_monthly %>% 
      group_by(article) %>% 
      mutate(rank=1:n()) %>% 
      tidyr::nest() %>% 
      mutate(data1=purrr::map(data,~filter(.x,rank<=3))) %>% 
      mutate(data1=purrr::map(data1,~summarise(.x, n_views_f3months=sum(nviews))),
             data2=purrr::map(data,~summarise(.x, n_views_tot=sum(nviews))),
             data3=purrr::map(data,~summarise(.x, n_months=n()))) %>% 
      select(-data) %>% 
      unnest(c(data1,data2,data3)) %>% 
      mutate(n_views_mean=n_views_tot/n_months)
    
    wm_full=readRDS("data/wm_full.RDS") %>%
      select(flood,deathtoll,date,date_precision,date_from) %>%
      unique()
    wp_pages_complete=readRDS("data/wp_pages_curation.RDS") %>% 
      left_join(wp_views_tot,by=c("article")) %>% 
      left_join(wm_full,by="flood") %>% 
      mutate(mean_views_longterm=(n_views_tot-n_views_f3months)/(n_months-3))
    saveRDS(wp_pages_complete,"data/wp_pages_complete.RDS")
}
wp_pages_complete=readRDS("data/wp_pages_complete.RDS")
```


```{r}
ggplot(wp_pages_complete,aes(x=n_views_f3months,y=curation, col=local))+
  geom_point()+
  scale_x_log10()+scale_y_log10()+
  geom_smooth()
```

```{r views_to_deathtoll,fig.width=10,fig.height=4}
p1=ggplot(wp_pages_complete, aes(x=local,n_views_mean, fill=local))+
  geom_boxplot()+
  scale_y_log10()
p2=ggplot(wp_pages_complete,aes(x=deathtoll,y=n_views_f3months, col=local))+
  geom_point()+
  scale_x_log10()+scale_y_log10()+
  geom_smooth(method="lm")


p3=ggplot(wp_pages_complete,aes(x=length,y=mean_views_longterm, col=local))+
  geom_point()+
  scale_x_log10()+scale_y_log10()+
  geom_smooth(method="lm")
figure <- ggpubr::ggarrange(p1,p2,p3,
                    labels = c("A", "B","C"),
                    ncol = 3, nrow = 1)
figure
```

```{r}
ggplot(wp_pages_complete,aes(x=n_months,y=curation, col=local))+
  geom_point()+scale_y_log10()+
  geom_smooth(method="lm")
```

